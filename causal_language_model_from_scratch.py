# -*- coding: utf-8 -*-
"""Causal language model from scratch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14efbK1MqD8gSsBW1QBnI-iice7hqhyWA

referance :
https://chatgpt.com/share/67c71226-af5c-8008-9b75-9032b284d4d2
"""

!pip install datasets

from collections import defaultdict
from datasets import Dataset
from transformers import AutoTokenizer ,AutoModelForCausalLM, Trainer, TrainingArguments ,AutoConfig ,GPT2LMHeadModel ,DataCollatorForLanguageModeling
from huggingface_hub import notebook_login

from torch.nn import CrossEntropyLoss
import torch
from torch.utils.data.dataloader import DataLoader

from datasets import load_dataset

split = "train"  # "valid"
filters = ["pandas", "sklearn", "matplotlib", "seaborn"]

data = load_dataset(f"transformersbook/codeparrot-{split}", split=split, streaming=True)

data

#hide_output
from transformers import pipeline, set_seed

generation_gpt = pipeline("text-generation", model="openai-gpt")
generation_gpt2 = pipeline("text-generation", model="gpt2")

from datasets import load_dataset, DatasetDict

ds_train = load_dataset("huggingface-course/codeparrot-ds-train", split="train")
ds_valid = load_dataset("huggingface-course/codeparrot-ds-valid", split="validation")

raw_datasets = DatasetDict(
    {
        "train": ds_train.shuffle().select(range(50000)),
        "valid": ds_valid.shuffle().select(range(500))
    }
)

raw_datasets

model="openai-gpt"
from transformers import AutoTokenizer, AutoModelForCausalLM
tokenizer = AutoTokenizer.from_pretrained(model)
model = AutoModelForCausalLM.from_pretrained(model)

list(model.named_parameters())[:12]  # Convert the generator to list and select 12 elements

def any_keyword_in_string(string, keywords):
    for keyword in keywords:
      if keyword in string:
        return True
    return False

string = "import pandas as pd"
filters = ["pandas", "sklearn", "matplotlib", "seaborn"]
any_keyword_in_string(string, filters)

ds_train

raw_datasets

for key in raw_datasets["train"][0]:
  print(f"{key}: {raw_datasets['train'][3][key]},'\n'")

"""# Preparing the dataset

##### The first step will be to tokenize the data, so we can use it for training. Since our goal is to mainly autocomplete short function calls, we can keep the context size relatively small. This has the benefit that we can train the model much faster and it requires significantly less memory. If it is important for your application to have more context (for example, if you want the model to write unit tests based on a file with the function definition), make sure you increase that number, but also keep in mind that this comes with a greater GPU memory footprint. For now, letâ€™s fix the context size at 128 tokens, as opposed to the 1,024 or 2,048 used in GPT-2 or GPT-3, respectively.
"""

context_length = 128

tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")

output = tokenizer(raw_datasets["train"][:2]["content"],
                   truncation = True,
                   max_length = context_length,
                   return_overflowing_tokens = True,
                   return_length = True)

print(f"Input IDs length: {len(output['input_ids'])}")
print(f"Chunk lengths: {output['length']}")
print(f"Chunk mapping: {output['overflow_to_sample_mapping']}")

output

def tokenize(elements):
  outputs = tokenizer(
      elements["content"],
      truncation=True,
      max_length=context_length,
      return_overflowing_tokens=True,
      return_length=True,
  )

  input_batches = []
  for length ,input_ids in zip(outputs["length"],outputs["input_ids"]):
    if length == context_length:
      input_batches.append(input_ids)

  return {"input_ids": input_batches}

tokenized_datasets = raw_datasets.map(
    tokenize, batched=True, remove_columns=raw_datasets["train"].column_names
)
tokenized_datasets

"""We now have **`500000  examples with 128 tokens each`**, which corresponds to about 2.1 billion tokens in total. For reference, OpenAIâ€™s GPT-3 and Codex models are trained on 300 and 100 billion tokens, respectively, where the Codex models are initialized from the GPT-3 checkpoints. Our goal in this section is not to compete with these models, which can generate long, coherent texts, but to create a scaled-down version providing a quick autocomplete function for data scientists.

## Initializing a new model from gpt2 config but not pretrained so we use config
"""

config = AutoConfig.from_pretrained(
    "gpt2",
    voca_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id = tokenizer.bos_token_id,
    eos_token_id = tokenizer.eos_token_id,
)

model = GPT2LMHeadModel(config)
model_size = sum( t.numel() for t in model.parameters())

print (f"GPT-2 size : {model_size/1000**2:.1f}M parameters")

"""# Data Collator"""

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer,mlm= False)

"""# let's do simple examples"""

out = data_collator([tokenized_datasets["train"][i] for i in range(5)])
for key in out:
  print(f"{key} shape: {out[key].shape}")

"""### We can see that the examples have been `stacked` and all the tensors have the same shape.
#### `Shifting the inputs and labels to align them happens inside the model`, so the data collator just copies the inputs to create the labels.

## intial notebook in HF
"""

notebook_login()

"""# API Trainer from HF"""

args = TrainingArguments(
    output_dir = "codeParrot-ds",
    per_device_train_batch_size = 32,
    per_device_eval_batch_size = 32,
    evaluation_strategy = "steps",
    eval_steps = 5_000,
    logging_steps = 5_000,
    gradient_accumulation_steps = 8,
    num_train_epochs = 1 ,
    weight_decay = 0.1,
    warmup_steps = 1_000,
    lr_scheduler_type = "cosine",
    learning_rate = 5e-4,
    save_steps = 5_000,
    fp16 = True,
    push_to_hub = True,
)

trainer = Trainer(
    model = model,
    tokenizer = tokenizer,
    args = args,
    data_collator = data_collator,
    train_dataset = tokenized_datasets["train"],
    eval_dataset = tokenized_datasets["valid"],

)

trainer.train()

trainer.push_to_hub()

"""# inferance"""

import torch
form transformers import pipline
model =
device = "cuda0" if torch.cuda.is_available() else "cpu"
pipe = pipeline("text-generation" , model = model ,device = device)

code = """\
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create scatter plot with x, y
"""

print (pipe(code, num_return_sequences=1)

print (pipe(code, num_return_sequences=1)[0]["generated_text"]

txt = """
# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# fit random forest model with 300 estimators on X, y:
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])

"""### the answer is the next cell"""

# # import random forest regressor from scikit-learn
# from sklearn.ensemble import RandomForestRegressor

# # fit random forest model with 300 estimators on X, y:
# rf = RandomForestRegressor(n_estimators=300, random_state=random_state, max_depth=3)
# rf.fit(X, y)
# rf

"""# Customized using << Training with ðŸ¤— Accelerate using pytorch

### Weâ€™ve seen how to train a model with the Trainer, which can allow for some customization. However, sometimes we want full control over the training loop, or we want to make some exotic changes. In this case ðŸ¤— Accelerate is a great choice, and in this section weâ€™ll go through the steps to use it to train our model. To make things more interesting, weâ€™ll also add a twist to the training loop.

### Since we are mainly interested in sensible autocompletion for the the data science libraries, it makes sense to give more weight to training samples that make more use of these libraries. We can easily identify these examples through the use of keywords such as plt, pd, sk, fit, and predict, which are the most frequent import names for matplotlib.pyplot, pandas, and sklearn as well as the fit/predict pattern of the latter.
"""

keytoekn-ids = []
for keyword in [
    "plt",
    "pd",
    "sk",
    "fit",
    "predict",
    " plt",
    " pd",
    " sk",
    " fit",
    " predict",
    "testtest",
]
ids = tokenizer([keyword]).input_ids[0]
if len(ids) == 1:
  keytoekn-ids.append(ids[0])

"""# Customized Loss **`Cross Entropy`**"""

# def keytoken_loss(inputs, logits,keytoken_id ,alpha =0.1):
#   shift_label = inputs[...,1:].contiguous()
#   shift_logit = logits[...,:-1,:].contiguous()

#   loss_fct = CrossEntropyLoss(reduce = False)
#   loss = loss_fct(shift_logit.view(-1,shift_logit.size(-1)),shift_label.view(-1))

#   loss_per_sample = loss.view(shift_logit.size(0),shift_logits.size(1)).mean(axis=0)

#   weights = torch.stack([(inputs == kt) for kt in keytoekn-ids]).sum(axis=[0,2])
#   wights = alpha *(1.0 + weights)

#   weighted_loss = loss_per_sample * weights
#   return weighted_loss.mean()

"""## or you can use `permute` as show the next instead the `view` function"""

def keytoken_loss(inputs, logits, keytoken_ids, alpha=0.1):
  shift_labels = inputs[: ,1:].contiguous()  #(batch_size,context_length)
  shift_logits = logits[: ,:-1,:].contiguous()  #(batch_size,context_length,vocab_size)


  loss_fct = CrossEntropyLoss(reduce=False)  #excpect (batch_size ,vocab_Size ,context_length)

  loss = loss_fct(shift_logits.permute(0,2,1),shift_labels) #(batch_size,context_length)

  loss_per_sample = loss.mean(axis=1) #(batch_
  weights = torch.stack([(inputs == kt) for kt in keytoken_ids]).sum(axis=[0,2])
  weights = alpha * (1.0 + weights)

  weighted_loss = (loss_per_sample * weights ).mean()
  return weighted_loss.mean()

"""# simulation example for all :"""

# def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):
#     """
#     Computes a weighted loss where tokens in `keytoken_ids` get higher importance.
#     """
#     # Step 1: Shift labels (targets) to the right
#     shift_labels = inputs[..., 1:].contiguous()  # Remove the first token
#     shift_logits = logits[..., :-1, :].contiguous()  # Remove the last token's logits

#     # Step 2: Compute cross-entropy loss per token (without flattening)
#     loss_fct = CrossEntropyLoss(reduction='none')  # Per-token loss
#     loss = loss_fct(shift_logits.permute(0, 2, 1), shift_labels)  # No need for view()


#     print("Loss Shape:", loss.shape)  # Debug print
#     print("Inputs Shape:", inputs.shape)  # Debug print
#     print("Shift Labels Shape:", shift_labels.shape)  # Debug print
#     print("Shift Logits Shape:", shift_logits.shape)  # Debug print


#     # Step 3: Compute mean loss per sample
#     loss_per_sample = loss.mean(dim=1)  # Averaging over sequence length
#     print ("loss_per_sample",loss_per_sample.shape)
#     print ("loss_per_sample",loss_per_sample)

#     # Step 4: Compute weights for key tokens
#     weights_withoutsum = torch.stack([(inputs == kt).float() for kt in keytoken_ids])
#     weightswith_sum = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(axis=[0,2])
#     print ("weights_withoutsum",weights_withoutsum.shape)
#     print ("weights_withoutsum",weights_withoutsum)
#     print ("weightswith_sum",weightswith_sum.shape)
#     print ("weightswith_sum",weightswith_sum)


#     print("Before weights", weights)
#     weights = alpha * (1.0 + weights)  # Ensure all samples contribute
#     print("after weights",weights)

#     # # Step 5: Compute final weighted loss
#     weighted_loss = (loss_per_sample * weights).mean()
#     print ("weighted_loss",weighted_loss)

#     # return weighted_loss
#     pass

# # Simulating input tokenized sequences (batch_size=2, seq_length=5)
# inputs = torch.tensor([
#     [1, 4, 3, 5, 2],  # Sequence 1
#     [2, 5, 7, 1, 3]   # Sequence 2
# ])

# # Simulating logits (batch_size=2, seq_length=5, vocab_size=10)
# logits = torch.randn(2, 5, 10)  # Random predictions from a model

# print ("Ologits ",logits)
# # Define key tokens we want to emphasize
# keytoken_ids = torch.tensor([3, 5])  # Tokens "3" and "5" are important  ( 1,2 )

# print ("keytoken_ids ",keytoken_ids)

# # Compute weighted loss
# loss_value = keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.5)
# # print("Weighted Loss:", loss_value.item())

"""### Before we can start training with this awesome new loss function, we need to prepare a few things:

- We need dataloaders to load the data in batches.
- We need to set up weight decay parameters.
- From time to time we want to evaluate, so it makes sense to wrap the evaluation code in a function.
"""

tokenized_datasets = tokenized_dataset.set_format("torch")
train_dataloader = DataLoader(tokenized_datasets["train"], shuffle=True, batch_size=32)
eval_dataloader = DataLoader(tokenized_datasets["valid"], batch_size=32)

"""### Next, we group the parameters so that the optimizer knows which ones will get an additional weight decay. Usually, all bias and LayerNorm weights terms are exempt from this"""

wight_decay = 0.1  # (L2 regularization) penalizes large weights to prevent overfitting It modifies the gradient update during training
no_decay = ["bias", "LayerNorm.weight"]

def group_parameters(model,weight_decay,no_decay):
  grouped_parameters = [
      {
          "params": [ p for n ,p in model.named_parameters() if not any (nd in n for nd in no_decay)],
          "weight_decay": 0.0
      },
      {
          "params": [ p for n ,p in model.named_parameters() if any (nd in n for nd in no_decay)],
          "weight_decay": weight_decay
      }
  ]
  return grouped_parameters

"""- ## Why Skip Biases & LayerNorm Weights?
- #### Bias terms (bias): Do not benefit from weight decay since they do not control scale.
- #### LayerNorm weights (LayerNorm.weight): These control normalization and should not be regularized, as it can harm training stability.
## `If we applied weight decay to all parameters, it could hurt performance.`

- âœ… Better` Generalization` â€“ `Prevents overfitting by decaying only relevant parameters.`
- âœ… Stable Training â€“ Avoids unnecessary `regularization of biases & normalization layers`.
- âœ… Standard Practice â€“ Used in Transformer-based models like `BERT & GPT`.
"""

model = GPT2LMHeadModel(config)

from torch.optim import AdamW
optimizer = AdamW(group_parameters(model,weight_decay,no_decay),lr = 5e-4)

"""

```
 What Happens Here?

```

- The optimizer gets two parameter groups:
- Parameters with weight decay (weight_decay=0.1).
- Parameters without weight decay (weight_decay=0.0 for biases & LayerNorm).
"""

from accelerate import Accelerator
accelerator = Accelerator(fp16=True)

model , optimizer , train_dataloader , eval_dataloader = accelerator.prepare(model,optimizer,train_dataloader,eval_dataloader)

"""# get_schedule"""

from transformers import get_scheduler


num_train_epochs = 1
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per


lr_scheduler = get_scheduler(
    name = "linear",
    optimizer = optimizer,
    num_warmup_steps = 1_000,
    num_training_steps = num_training_steps
)

def evaluate():
  model.eval()
  losses = []
  for step, batch in enumerate(eval_dataloader):
    with torch.no_grad():
      outputs = model(batch["input_ids"], labels = batch["input_ids"])
    loss = outputs.loss
    losses.append(accelerator.gather(loss))
  loss = torch.mean(torch.cat(loss))  # losses = [torch.tensor([1.2]), torch.tensor([1.4]), torch.tensor([1.3])] and cat make  it to tensor([1.2, 1.4, 1.3])

  try :
    perplexity = torch.exp(loss)
  except OverflowError:
    perplexity = torch.tensor(float("inf"))

  return loss.item(), perplexity.item()

import torch

n = torch.tensor(2.)

n

n.item()

from huggingface_hub import Repository, get_full_repo_name

model_name = "codeparrot-ds"
repo_name = get_full_repo_name(model_name)
repo_name

output_dir = "codeparrot-ds-accelerate"
repo = Repository(output_dir, clone_from=repo_name)

"""### Before we train, letâ€™s run a quick test to see if the evaluation function works properly:

"""

evaluate()

from tqdm.auto import tqdm



gradient_accumulation_steps = 8

eval_steps = 5_000


model.train()

completed_steps = 0
for epoch in range(num_train_epochs):
  for step, batch in tqdm(enumerate(train_dataloader, start=1), total=num_training_steps):
    logits = model(batch["input_ids"]).logits
    loss = keytoken_loss(batch["input_ids"], logits, keytoken_ids)

    if step % 100 ==0:
      accelerator.print(
                {
                    "samples": step * samples_per_step,
                    "steps": completed_steps,
                    "loss/train": loss.item() * gradient_accumulation_steps,
                }
            )

    loss = loss / gradient_accumulation_steps
    accelerator.backward(loss)
    if step % gradient_accumulation_steps == 0:
      accelerator.clip_grad_norm_(model.parameters(), 1.0)
      optimizer.step()
      lr_scheduler.step()
      optimizer.zero_grad()
      completed_steps +=1

    if step % (eval_steps * gradient_accumulation_steps) == 0:
      eval_loss, perplexity = evaluate()
      accelerator.print({"loss/eval": eval_loss, "perplexity": perplexity})
      model.train()


      accelerator.wait_for_everyone()
      unwrapped_model = accelerator.unwrap_model(model)
      unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
      if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress step {step}", blocking=False
        )









